{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 1:  Modeling Text + Link Analysis + SEO\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Monday, February 8, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand the vector space model (TF-IDF, cosine) + BM25 works in searching. Explore real-world challenges of building a graph (in this case, from Epinions), implement and test the classic HITS algorithm over this graph. Experiment with real-world search engine optimization techniques.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via eCampus (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Modeling Text (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "\n",
    "First, you will need to download the review.json file from the Resources tab on Piazza, a collection of about 7,000 Yelp reviews we sampled from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). You'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. You need to load the json file first. We already have done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can treat each review as a document. Given a query, you need to calculate its TF-IDF score in each review.  For this homework, we will define the TF-IDF as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TF = number of times word occurs in a document`\n",
    "\n",
    "`IDF = log(total number of documents / number of documents containing the word)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Ranking with simple sums of TF-IDF scores\n",
    "\n",
    "To start out with, for a multi-word query, we will rank documents by a simple sum of the TF-IDF scores for the query terms in the document. \n",
    "\n",
    "Please calculate this TF-IDF sum score for queries `\"best bbq\"` and `\"kid fun and food\"`. You need to report the Top-10 reviews with highest TF-IDF scores for each query. Your output should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query \"best bbq\"\n",
    "\n",
    "Rank Review_ID score\n",
    "\n",
    "1 dhskfhjskfhs 0.55555\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "Query \"kid fun and food\"\n",
    "\n",
    "Rank Review_ID score\n",
    "\n",
    "1 dhskfhjskfhs 0.55555\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Load json file\n",
    "# Read each review\n",
    "# Tokenize it using whitespace\n",
    "# Increment respective counters \n",
    "# Column names would be review IDs\n",
    "\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "\n",
    "total_docs =0\n",
    "DF=[0,0,0,0,0,0]\n",
    "\n",
    "with open('review.json') as f:\n",
    "    dict_TF1 = {}\n",
    "    dict_TF2 = {}\n",
    "    \n",
    "    for line in f:\n",
    "        total_docs += 1\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        a = data['review'].split(' ')\n",
    "        \n",
    "        TF=[]\n",
    "        TF.append(a.count(\"best\"))\n",
    "        TF.append(a.count(\"bbq\"))\n",
    "        TF.append(a.count(\"kid\"))\n",
    "        TF.append(a.count(\"fun\"))\n",
    "        TF.append(a.count(\"and\"))\n",
    "        TF.append(a.count(\"food\"))\n",
    "        \n",
    "        dict_TF1[data['id']] = [TF[0], TF[1]]\n",
    "        dict_TF2[data['id']] = [TF[2], TF[3], TF[4], TF[5]]\n",
    "        \n",
    "        for i in range(6):\n",
    "            if(TF[i]>0):\n",
    "                DF[i] += 1\n",
    "#print(DF)\n",
    "final_dict = {}\n",
    "for key in dict_TF1:\n",
    "    val = 0\n",
    "    val = dict_TF1[key][0] * math.log10(total_docs/DF[0])\n",
    "    val += dict_TF1[key][1] * math.log10(total_docs/DF[1])\n",
    "    final_dict[key] = val\n",
    "\n",
    "final_dict_1 = {}\n",
    "for key in dict_TF2:\n",
    "    val = 0\n",
    "    val = dict_TF2[key][0] * math.log10(total_docs/DF[2])\n",
    "    val += dict_TF2[key][1] * math.log10(total_docs/DF[3])\n",
    "    val += dict_TF2[key][2] * math.log10(total_docs/DF[4])\n",
    "    val += dict_TF2[key][3] * math.log10(total_docs/DF[5])\n",
    "    final_dict_1[key] = val    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query \"best bbq\"\n",
      "Rank Review_ID score\n",
      "1 YbQvHNrjZJ38mnh5rLuq7w 11.430532931561633\n",
      "2 P31kXP4oan6ZQm69TN6tIA 9.525444109634693\n",
      "3 x5esEK6J9XkA_vbvVbG8Gg 8.471542878759161\n",
      "4 mWs26TrBM7ogwCM9UfVJFg 7.6203552877077545\n",
      "5 NCfX4AxDvQ3QRyXKtmhVwQ 7.6203552877077545\n",
      "6 e5INq6DAZn2zMHicKQl07Q 6.566454056832223\n",
      "7 4WTG1-9mw8YHEyaTu8dQww 6.566454056832223\n",
      "8 x3n_l3GhBx78y6jWX4fStg 5.958313137359845\n",
      "9 Wp8jYXL1DQrgrnZIFmufFg 5.715266465780816\n",
      "10 jrEx93eYKIjCW2nrkwjZpQ 5.715266465780816\n"
     ]
    }
   ],
   "source": [
    "# Show us the result for \"best bbq\"\n",
    "x = final_dict\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "print(\"Query \\\"best bbq\\\"\")\n",
    "print(\"Rank Review_ID score\")\n",
    "for i in range(10):\n",
    "    print(i+1, sorted_x[i][0], sorted_x[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query \"kid fun and food\"\n",
      "Rank Review_ID score\n",
      "1 7o_hciiXEMNQkXfVl0F0XQ 9.641872501183393\n",
      "2 JKLUXUovJCU6kbcdin74NQ 8.692007941255747\n",
      "3 IA8TOfGKI-Il-70BsB6HgA 8.132369151667877\n",
      "4 Kytq1NbFIDDCXUculSqT8g 7.2912649956941475\n",
      "5 MF6rPRx9jz-g8S5P_ZIdyg 7.080045177182006\n",
      "6 bjoedmJ4_DZP5JnfXVaC-w 6.825484846867428\n",
      "7 I00B-QG5uTKvwCK7x9ejeA 6.8021590858855445\n",
      "8 BVGRJgDJGEhSfgIPCan7vQ 6.721602275668411\n",
      "9 wMB3cI3-xhxM_BpmppY9RQ 6.425196423168579\n",
      "10 vTGDEQGp6EPlwdMJUnTb7A 6.0414680741761755\n"
     ]
    }
   ],
   "source": [
    "# Show us the result for \"kid fun and food\"\n",
    "x = final_dict_1\n",
    "print(\"Query \\\"kid fun and food\\\"\")\n",
    "print(\"Rank Review_ID score\")\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "for i in range(10):\n",
    "    print(i+1, sorted_x[i][0], sorted_x[i][1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Ranking with TF-IDF + Cosine\n",
    "\n",
    "Instead of using the sum of TF-IDF scores, let's try the classic cosine approach for ranking. You should still use the TF-IDF scores to weigh each term, but now use the cosine between the query vector and the document vector to assign a similarity score. You can try the same two queries as before and report the results. (Top-10 reviews which are similar to the query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best 951\n",
      "bbq 84\n",
      "kid 87\n",
      "fun 337\n",
      "and 5990\n",
      "food 1854\n",
      "total no of words  24172\n",
      "total doc count  6751\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Treat\n",
    "\n",
    "TF_globally = {}\n",
    "# This has all the TFs in each document\n",
    "# To calculate the normalization. The tf-idf of each term in the document is needed\n",
    "\n",
    "DF_globally = {}\n",
    "# Has all the counts for the terms across documents\n",
    "\n",
    "with open('review.json') as f:    \n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        a = data['review'].split()\n",
    "        len_of_doc = len(a) \n",
    "        \n",
    "        unique_dict = {}\n",
    "        for i in range(len_of_doc):\n",
    "            if a[i] in unique_dict:\n",
    "                unique_dict[a[i]] += 1\n",
    "            else:\n",
    "                unique_dict[a[i]] = 1\n",
    "        TF_globally[data['id']] = unique_dict\n",
    "        \n",
    "        for key in unique_dict:\n",
    "            temp_word = key\n",
    "            #print(temp_word)\n",
    "            if temp_word in DF_globally:\n",
    "                DF_globally[temp_word] += 1\n",
    "            else:\n",
    "                DF_globally[temp_word] = 1\n",
    "\n",
    "ter = ['best','bbq','kid','fun','and','food']\n",
    "for t in ter:\n",
    "    print(t, DF_globally[t])\n",
    "#print(DF_globally['best'], DF_globally['bbq'], DF_globally['kid'], DF_globally['fun'], DF_globally['and'], DF_globally['food'])\n",
    "\n",
    "total_docs = len(TF_globally)\n",
    "print('total no of words ',len(DF_globally))\n",
    "print('total doc count ', total_docs)\n",
    "\n",
    "def TFIDF_Doc_Mod(docid):\n",
    "    terms = TF_globally[docid]\n",
    "    tfidf = 0\n",
    "    for t in terms:\n",
    "        tf = terms[t]\n",
    "        idf = math.log10(total_docs/DF_globally[t])\n",
    "        tfidf += tf*idf*tf*idf\n",
    "    return math.sqrt(tfidf)\n",
    "\n",
    "def TFIDF_Query_Mod(terms1):\n",
    "    tfidf = 0\n",
    "    for t in terms1:\n",
    "        tf=1\n",
    "        idf = math.log10(total_docs/DF_globally[t])\n",
    "        tfidf += tf*idf*tf*idf\n",
    "    return math.sqrt(tfidf)\n",
    "\n",
    "def IDF_term(term):\n",
    "    return math.log10(total_docs/DF_globally[term])\n",
    "\n",
    "final_dict = {}\n",
    "ter = ['best', 'bbq']\n",
    "for key in TF_globally:\n",
    "    dotpdt = 0\n",
    "    for t in ter:\n",
    "        if t not in TF_globally[key]:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = TF_globally[key][t]\n",
    "        tf_doc = val\n",
    "        idf_doc = IDF_term(t)\n",
    "        idf_query = IDF_term(t)\n",
    "        dotpdt += tf_doc*idf_doc*idf_query\n",
    "    final_dict[key] = dotpdt / ( TFIDF_Doc_Mod(key) * TFIDF_Query_Mod(ter))\n",
    "\n",
    "final_dict_1 = {}\n",
    "ter = ['kid','fun','and','food']\n",
    "for key in TF_globally:\n",
    "    dotpdt = 0\n",
    "    for t in ter:\n",
    "        if t not in TF_globally[key]:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = TF_globally[key][t]\n",
    "        tf_doc = val\n",
    "        idf_doc = IDF_term(t)\n",
    "        idf_query = IDF_term(t)\n",
    "        dotpdt += tf_doc*idf_doc*idf_query\n",
    "    final_dict_1[key] = dotpdt / ( TFIDF_Doc_Mod(key) * TFIDF_Query_Mod(ter))\n",
    "#[951, 84, 87, 337, 5990, 1854]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query \"best bbq\"\n",
      "Rank Review_ID score\n",
      "1 x5esEK6J9XkA_vbvVbG8Gg 0.5317240946819429\n",
      "2 P31kXP4oan6ZQm69TN6tIA 0.46188402591085276\n",
      "3 8p-KEtrrTmLv-o1mKpUy1A 0.43897268486712104\n",
      "4 _fNfowXaxXcYChKukMrYeg 0.3979856739671412\n",
      "5 NCfX4AxDvQ3QRyXKtmhVwQ 0.3665421991949398\n",
      "6 4iCl2qJaz9GPaU4v5bRW2A 0.36308346241131384\n",
      "7 HzNxErSCQ2FYfPCbyfHrSQ 0.36237683074178917\n",
      "8 e5INq6DAZn2zMHicKQl07Q 0.33485418089677293\n",
      "9 Wp8jYXL1DQrgrnZIFmufFg 0.3118555623309593\n",
      "10 1tJ_iJX_KZ3zM_9_GRaGTg 0.31082470147489855\n"
     ]
    }
   ],
   "source": [
    "# Show us the result for \"best bbq\"\n",
    "\n",
    "x = final_dict\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "print(\"Query \\\"best bbq\\\"\")\n",
    "print(\"Rank Review_ID score\")\n",
    "for i in range(10):\n",
    "    print(i+1, sorted_x[i][0], sorted_x[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query \"kid fun and food\"\n",
      "Rank Review_ID score\n",
      "1 IUME6cWFSwH1mSh_1_U81g 0.4104441171922874\n",
      "2 6xdziQ46TZWKBpKQPNCSGw 0.2897208925770315\n",
      "3 OExraycGW4VxL0Xth1xZ4w 0.2511025783811886\n",
      "4 RRGemWMJskG2VQiDzjAOhw 0.23813741108195638\n",
      "5 37RfMeDMo8QEVAF8yT31Ww 0.21661731670833084\n",
      "6 JKLUXUovJCU6kbcdin74NQ 0.21259362982108448\n",
      "7 rM_V3OfrwWA7vHsXsUmq2w 0.21146292901726388\n",
      "8 k7HxGMgabFxDUi2XWZ_hOg 0.20781006062485471\n",
      "9 5oLxygfaHo2dMf9dbRxc4w 0.19992966478169988\n",
      "10 XTSD0-Wi1r_k2EQOCpv8hA 0.19296521321517499\n"
     ]
    }
   ],
   "source": [
    "# Show us the result for \"kid fun and food\"\n",
    "x = final_dict_1\n",
    "print(\"Query \\\"kid fun and food\\\"\")\n",
    "print(\"Rank Review_ID score\")\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "for i in range(10):\n",
    "    print(i+1, sorted_x[i][0], sorted_x[i][1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Ranking with BM25\n",
    "\n",
    "Finally, let's try the BM25 approach for ranking. Refer to [https://en.wikipedia.org/wiki/Okapi_BM25](https://en.wikipedia.org/wiki/Okapi_BM25) for the specific formula. You should choose k_1 = 1.2 and b = 0.75. You need to report the Top-10 reviews with highest BM25 scores for each query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6751\n",
      "127.88268404680788\n",
      "doc len 6751\n",
      "[951, 84, 87, 337, 5990, 1854]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "total_docs =0\n",
    "DF=[0,0,0,0,0,0]\n",
    "avgdl = 0\n",
    "\n",
    "with open('review.json') as f:\n",
    "    dict_TF1 = {}\n",
    "    dict_TF2 = {}\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        total_docs += 1\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        a = data['review'].split()\n",
    "        \n",
    "        TF=[]\n",
    "        TF.append(a.count(\"best\"))\n",
    "        TF.append(a.count(\"bbq\"))\n",
    "        TF.append(a.count(\"kid\"))\n",
    "        TF.append(a.count(\"fun\"))\n",
    "        TF.append(a.count(\"and\"))\n",
    "        TF.append(a.count(\"food\"))\n",
    "        \n",
    "        lena= len(a)\n",
    "        avgdl += lena\n",
    "        \n",
    "        dict_TF1[data['id']] = [TF[0], TF[1], lena]\n",
    "        dict_TF2[data['id']] = [TF[2], TF[3], TF[4], TF[5], lena]\n",
    "        \n",
    "        for i in range(len(TF)):\n",
    "            if(TF[i]>0):\n",
    "                DF[i] += 1\n",
    "#print(DF)\n",
    "print(total_docs)\n",
    "avgdl = float(avgdl) / total_docs\n",
    "\n",
    "\n",
    "def IDF_qi(N, n_qi):\n",
    "    val = 0\n",
    "    #val = math.log10( (N - n_qi + 0.5) / (n_qi+0.5) )\n",
    "    val = math.log10(N/n_qi)\n",
    "    return val\n",
    "\n",
    "def DF_qi(fun,modD,avgdl):\n",
    "    #fun is term frequency of q_i\n",
    "    k_1 = 1.2\n",
    "    b = 0.75\n",
    "    val = 0\n",
    "    val = float(fun) *(2.2)\n",
    "    val = val / (fun + ( 1.2 * (0.25 + (0.75* modD/avgdl) ) ) )\n",
    "    return val\n",
    "\n",
    "print(avgdl)\n",
    "print('doc len', total_docs)\n",
    "final_dict = {}\n",
    "for key in dict_TF1:\n",
    "    val = 0\n",
    "    val = IDF_qi(total_docs, DF[0]) * DF_qi(dict_TF1[key][0], dict_TF1[key][-1], avgdl)\n",
    "    val += IDF_qi(total_docs, DF[1]) * DF_qi(dict_TF1[key][1], dict_TF1[key][-1], avgdl)\n",
    "    final_dict[key] = val\n",
    "\n",
    "print(DF)\n",
    "final_dict_1 = {}\n",
    "for key1 in dict_TF2:\n",
    "    val = 0\n",
    "    lis = dict_TF2[key1]\n",
    "    \n",
    "    #print(dict_TF2[key1], lis[-1])\n",
    "    val = IDF_qi(total_docs, DF[2]) * DF_qi(lis[0], lis[-1], avgdl)\n",
    "    val += IDF_qi(total_docs, DF[3]) * DF_qi(lis[1], lis[-1], avgdl)\n",
    "    val += IDF_qi(total_docs, DF[4]) * DF_qi(lis[2], lis[-1], avgdl)\n",
    "    val += IDF_qi(total_docs, DF[5]) * DF_qi(lis[3], lis[-1], avgdl)\n",
    "    final_dict_1[key1] = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query \"best bbq\"\n",
      "Rank Review_ID score\n",
      "1 x5esEK6J9XkA_vbvVbG8Gg 4.222154734433089\n",
      "2 xpm6TgDiHaQdEDlErFsqvQ 4.0889339452307425\n",
      "3 4WTG1-9mw8YHEyaTu8dQww 3.8910274585855507\n",
      "4 e5INq6DAZn2zMHicKQl07Q 3.731673460329797\n",
      "5 GASAd_gPBY_eWIL9XJwuNA 3.4642160116561844\n",
      "6 P31kXP4oan6ZQm69TN6tIA 3.4222718800768743\n",
      "7 8p-KEtrrTmLv-o1mKpUy1A 3.309119999963103\n",
      "8 HzNxErSCQ2FYfPCbyfHrSQ 3.229482350635871\n",
      "9 -RApX_RMzJLnpommDpQfKQ 3.2040958386956215\n",
      "10 1tJ_iJX_KZ3zM_9_GRaGTg 3.1239630631638624\n"
     ]
    }
   ],
   "source": [
    "# Show us the result for \"best bbq\"\n",
    "x = final_dict\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "print(\"Query \\\"best bbq\\\"\")\n",
    "print(\"Rank Review_ID score\")\n",
    "for i in range(10):\n",
    "    print(i+1, sorted_x[i][0], sorted_x[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query \"kid fun and food\"\n",
      "Rank Review_ID score\n",
      "1 kDwMMrSiB_AlV0erhVigFg 3.4403358410689635\n",
      "2 6xdziQ46TZWKBpKQPNCSGw 3.054596625538573\n",
      "3 UMqvuRtTxJFuWbgT6qO9cg 3.0236961472468282\n",
      "4 TVq6HhhJizKM1mReF9hvJQ 3.0133396564389154\n",
      "5 OExraycGW4VxL0Xth1xZ4w 3.012355781988829\n",
      "6 nuKIKXuQ51eRywuCcoX3fQ 2.9815155466598267\n",
      "7 k7HxGMgabFxDUi2XWZ_hOg 2.9796259517085786\n",
      "8 JKLUXUovJCU6kbcdin74NQ 2.96139640420127\n",
      "9 EDQzFQ7yYbRVUWCNA4rTOQ 2.9553169750109776\n",
      "10 BLQYsPFFAezpbbF-1dzD4Q 2.945529446180057\n"
     ]
    }
   ],
   "source": [
    "# Show us the result for \"kid fun and food\"\n",
    "\n",
    "x = final_dict_1\n",
    "print(\"Query \\\"kid fun and food\\\"\")\n",
    "print(\"Rank Review_ID score\")\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "for i in range(10):\n",
    "    print(i+1, sorted_x[i][0], sorted_x[i][1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the differences you see between the three methods. Is there one you prefer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive TF-IDF score calculation is simple but we aren't weighting the terms' occurence within a document. Cosine takes care of weights within the doc and also gives similarity across others. In BM25, the ranking is done with a probabilistic approach and has document length normalization.\n",
    "\n",
    "I would prefer BM25 due to its straightforward nature and its simplicity in computation unlike Cosine where the term count for the entire document is calculated and takes a lot of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Link Analysis (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Trust Graph\n",
    "\n",
    "\n",
    "In this part, we're going to adapt the classic HITS approach to allow us to find not the most authoritative web pages, but rather to find the most trustworthy users. [Epinions.com](https://snap.stanford.edu/data/soc-Epinions1.html) is a general consumer review site with a who-trust-whom online social network. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user. (Refer to: Richardson, Matthew, Rakesh Agrawal, and Pedro Domingos. \"Trust management for the semantic web.\" International semantic Web conference. Springer, Berlin, Heidelberg, 2003.)\n",
    "\n",
    "So, instead of viewing the world as web pages with hyperlinks (where pages = nodes, hyperlinks = edges), we're going to construct a graph of Epinions users and their \"trust\" on other users (so user = node, trust behavior = edge). Over this Epinions-user graph, we can apply the HITS approach to order the users by their hub-ness and their authority-ness.\n",
    "\n",
    "You need to download the *Epinions_trust.txt* file from the Resources tab on Piazza. Each line represents the trust relationship between two users. Here is a toy example. Suppose you are given the following four lines:\n",
    "\n",
    "* diane trust bob\n",
    "* charlie trust bob \n",
    "* charlie trust alice\n",
    "* bob trust charlie\n",
    "\n",
    "The \"trust\" between each user pair denotes a directed edge between two nodes. E.g., the \"diane\" node has a directed edge to the \"bob\" node (as indicated by the first line). \n",
    "\n",
    "You should build a graph by parsing the data in the file we provide called *Epinions_trust.txt*. (Note: The edges are binary and directed.)\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* The edges are binary and directed.\n",
    "* User can't trust himself/herself.\n",
    "* Later you will need to implement the HITS algorithm on the graph you build here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of nodes = 658\n",
      "no of edges = 6392\n"
     ]
    }
   ],
   "source": [
    "# Here define your function for building the graph \n",
    "# by parsing the input file \n",
    "# Insert as many cells as you want\n",
    "\n",
    "nodes = {}\n",
    "hubs_scores = {}\n",
    "auth_scores = {}\n",
    "in_links = {}\n",
    "out_links = {}\n",
    "edges = 0\n",
    "with open('Epinions_trust.txt') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        edges+=1\n",
    "        data = line.split()\n",
    "        \n",
    "        \n",
    "        x = data[0]\n",
    "        y = data[2]\n",
    "        #print(data)\n",
    "        if x not in out_links:\n",
    "            out_links[x] = [y]\n",
    "        else:\n",
    "            out_links[x].append(y) \n",
    "        \n",
    "        if y not in in_links:\n",
    "            in_links[y] = [x]\n",
    "        else:\n",
    "            in_links[y].append(x)\n",
    "        \n",
    "        if x not in nodes:\n",
    "            nodes[x] = 1\n",
    "            hubs_scores[x] = 1\n",
    "            auth_scores[x] = 0\n",
    "        if y not in nodes:\n",
    "            nodes[y]=1\n",
    "            hubs_scores[y] = 1\n",
    "            auth_scores[y] = 0\n",
    "\n",
    "print('no of nodes =',len(nodes))\n",
    "print('no of edges =',edges)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please show us the size of the graph, i.e., the number of nodes and edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hub scores\n",
      "charles - 0.01077806784830701\n",
      "teanna3 - 0.010601698998844594\n",
      "JediKermit - 0.009706597287501123\n",
      "melissasrn - 0.009204463382541784\n",
      "KCFemme - 0.008956781907580329\n",
      "missi31 - 0.008510913584527354\n",
      "jeanniekerns - 0.00847439019789119\n",
      "jag2112 - 0.008439704917722625\n",
      "mrssmoopy - 0.008423488603918521\n",
      "briandalsmom - 0.008277345654336339\n",
      "\n",
      "Authority scores\n",
      "melissasrn - 0.024555174015973746\n",
      "shantel575 - 0.01920337545571596\n",
      "surferdude7 - 0.01900004487151817\n",
      "sblaydes - 0.015394651149068073\n",
      "tiffer0220 - 0.015179508661659055\n",
      "opinionated3 - 0.0149316061393354\n",
      "patch3boys - 0.012795881153539487\n",
      "merlot - 0.012637886366063417\n",
      "pogomom - 0.012353971321107162\n",
      "chrisceb - 0.011722581225198545\n"
     ]
    }
   ],
   "source": [
    "# Call your function to print out the size of the graph\n",
    "# How you maintain the graph is totally up to you\n",
    "\n",
    "no_of_iterations = 5\n",
    "\n",
    "def calc_auth():\n",
    "    norma = 0\n",
    "    for key in auth_scores:\n",
    "        if key in in_links:\n",
    "            in1 = in_links[key]\n",
    "            temp = 0\n",
    "            for j in in1:\n",
    "                temp += hubs_scores[j]\n",
    "            auth_scores[key] = temp\n",
    "        norma += auth_scores[key]\n",
    "    \n",
    "    for key in auth_scores:\n",
    "        auth_scores[key] = auth_scores[key] / norma\n",
    "def calc_hubs():\n",
    "    norma = 0\n",
    "    for key in hubs_scores:\n",
    "        if key in out_links:\n",
    "            out1 = out_links[key]\n",
    "            temp = 0\n",
    "            for j in out1:\n",
    "                temp += auth_scores[j]\n",
    "            hubs_scores[key] = temp\n",
    "        norma += hubs_scores[key]\n",
    "    \n",
    "    for key in hubs_scores:\n",
    "        hubs_scores[key] = hubs_scores[key] / norma\n",
    "\n",
    "for i in range(no_of_iterations):\n",
    "    calc_auth()\n",
    "    calc_hubs()\n",
    "\n",
    "import operator\n",
    "print(\"Hub scores\")\n",
    "x = hubs_scores\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "for i in range(10):\n",
    "    print(sorted_x[i][0], '-', sorted_x[i][1])  \n",
    "\n",
    "print(\"\\nAuthority scores\")\n",
    "x = auth_scores\n",
    "sorted_x = sorted(x.items(), reverse=True, key=operator.itemgetter(1))\n",
    "for i in range(10):\n",
    "    print(sorted_x[i][0], '-', sorted_x[i][1])          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITS Implementation\n",
    "\n",
    "Your program will return the top 10 users with highest hub and authority scores. The **output** should be like:\n",
    "\n",
    "Hub Scores\n",
    "\n",
    "* user1 - score1\n",
    "* user2 - score2\n",
    "* ...\n",
    "* user10 - score10\n",
    "\n",
    "Authority Scores\n",
    "\n",
    "* user1 - score1\n",
    "* user2 - score2\n",
    "* ...\n",
    "* user10 - score10\n",
    "\n",
    "You should follow these **rules**:\n",
    "\n",
    "* Assume all nodes start out with equal scores.\n",
    "* It is up to you to decide when to terminate the HITS calculation.\n",
    "* There are HITS implementations out there on the web. However, remember, your code should be **your own**.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "* If you're using the matrix style approach, you should use [numpy.matrix](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.html).\n",
    "* Scipy is built on top of Numpy and has support for sparse matrices. You most likely will not need to use Scipy unless you'd like to try out their sparse matrices.\n",
    "* If you choose to use Numpy (and Scipy), please make sure your Anaconda environment include their latest versions.\n",
    "* Test your parsing and HITS calculations using a handful of trust relationships, before moving on to the entire file we provide.\n",
    "* We will evaluate the user ranks you provide as well as the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Search Engine Optimization (10 + 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, your goal is to put on your \"[search engine optimization](https://en.wikipedia.org/wiki/Search_engine_optimization)\" hat. Your job is to create a webpage that scores highest for the query: **sajfd hfafbjhd** --- two terms, lower case, no quote. As of today (Jan 24, 2019), there are no hits for this query on either Google or Bing. Based on our discussions of search engine ranking algorithms, you know that several factors may impact a page's rank. Your goal is to use this knowledge to promote your own page to the top of the list.\n",
    "\n",
    "What we're doing here is a form of [SEO contest](https://en.wikipedia.org/wiki/SEO_contest). While you have great latitude in how you approach this problem, you are not allowed to engage in any unethical or illegal behavior. Please read the discussion of \"white hat\" versus \"black hat\" SEO over at [Wikipedia](https://en.wikipedia.org/wiki/Search_engine_optimization).\n",
    "\n",
    "\n",
    "**Rules of the game:**\n",
    "\n",
    "* Somewhere in the page (possibly in the non-viewable source html) you must include your name or some other way for us to identify you.\n",
    "* Your target page may only be a TAMU student page, a page on your own webserver, a page on a standard blog platform (e.g., wordpress), or some other primarily user-controlled page\n",
    "* Your target page CAN NOT be a twitter account, a facebook page, a Yahoo Answers or similar page\n",
    "* No wikipedia vandalism\n",
    "* No yahoo/wiki answers questions\n",
    "* No comment spamming of blogs\n",
    "* If you have concerns/questions/clarifications, please post on Piazza and we will discuss\n",
    "\n",
    "For your homework turnin for this part, you should provide us the URL of your target page and a brief discussion (2-4 paragraphs) of the strategies you are using. We will issue the query and check the rankings at some undetermined time in the next couple of weeks. You might guess that major search engines take some time to discover and integrate new pages: if I were you, I'd get a target page up immediately.\n",
    "\n",
    "**Grading:**\n",
    "\n",
    "* 5 points for providing a valid URL\n",
    "* 5 points for a well-reasoned discussion of your strategy\n",
    "\n",
    "** Bonus: **\n",
    "* 1 point for your page appearing in the top-20 on Google or Bing\n",
    "* 1 more point for your page appearing in the top-10 on Google or Bing\n",
    "* 1 more point for your page appearing in the top-5 on Google or Bing\n",
    "* 2 more points for your page being ranked first by Google or Bing. And, a vigorous announcement in class, and a high-five for having the top result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the URL of your page?\n",
    "\n",
    "http://people.tamu.edu/~kavyasree.bvs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's your strategy? (2-4 paragraphs)\n",
    "\n",
    "I started off with a tamu.edu personal website. I then included the query term in the body of the website's content. To make it more readable, I have provided a brief description of the website, what it is meant for.\n",
    "\n",
    "I then went to include my website link in my LinkedIn and GitHub pages. This was to ensure, that the inlinks to my websites are coming from reputed sources. I then expanded it to creating a new place in google maps and including my website link in its description. Additionally, I had the opportunity to upload a YouTube video and point to my website from that video.\n",
    "\n",
    "I created outlinks to my github repository and youTube links to creat hub-authority equation. I then have written up a little story on iron man vs batman and included few images to mark my website as a genuine one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
